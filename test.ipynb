{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# DATA LAKE PROJECT FOR SPARKIFY\n",
    "-------------------------------------------------------------------------------\n",
    "Sparkify is a music streaming company. It has grown the user base. Currently, the data resides in S3 in JSON format on user activity of the app. As a data engineer, my task is to build ETL pipeline that extracts data from S3, process with Spark and loads back into S3 as a set of dimentional tables. This allows the analytics team to continue finding insights in what songs their users are listening to.  \n",
    "\n",
    "## DATASET\n",
    "- Song Data: `s3://udacity-dend/song_data`\n",
    "    - The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.\n",
    "    - `song_data/A/B/C/TRABCEI128F424C983.json`\n",
    "    - `song_data/A/A/B/TRAABJL12903CDCF1A.json`\n",
    "    - Raw data example:\n",
    "    `{\"num_songs\": 1, \"artist_id\": \"ARJIE2Y1187B994AB7\", \"artist_latitude\": null, \"artist_longitude\": null, \"artist_location\": \"\", \"artist_name\": \"Line Renaud\", \"song_id\": \"SOUPIRU12A6D4FA1E1\", \"title\": \"Der Kleine Dompfaff\", \"duration\": 152.92036, \"year\": 0}`\n",
    "\n",
    "- Log Data: `s3://udacity-dend/log_data`\n",
    "    - The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.\n",
    "    - `log_data/2018/11/2018-11-12-events.json`\n",
    "    - `log_data/2018/11/2018-11-13-events.json`\n",
    "    \n",
    "\n",
    "## RESULTANT STAR SCHEMA FOR FACT AND DIMENSION TABLE\n",
    "- Fact Table\n",
    "    - songplays - records in log data associated with song plays i.e. records with page NextSong: \n",
    "    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "    \n",
    "- Dimension Tables\n",
    "    - users - user_id, first_name, last_name, gender, level\n",
    "    - songs - song_id, title, artist_id, year, duration\n",
    "    - artists - artist_id, name, location, lattitude, longitude\n",
    "    - time - start_time, hour, day, week, month, year, weekday\n",
    "    \n",
    "## REQUIREMENT\n",
    "- Need to create IAM role with `programmatic` access and full access permission for S3.\n",
    "- Need to create EMR cluster with 3 nodes: 1 master and 2 slaves.\n",
    "    - EMR version: `emr-5.20.0` or later\n",
    "    - Applications: Spark 2.4.0 on Hadoop 2.8.5 YARN with Ganglia 3.7.2 and Zeppelin 0.8.0\n",
    "    - Instance type: m3.xlarge\n",
    "    - EC2 key pair required to connect with the master node to submit the spark job or one can run using the `Jupyter Notebook` provided by EMR in AWS portal.\n",
    "    \n",
    "## STEP BY STEP PROJECT EXECUTION\n",
    "\n",
    "- `dl.cfg`: Need to add access and secret key related to IAM user\n",
    "- Need to create S3 bucket for the output_path\n",
    "- `tl.py`has the code that loads data into cluster using spark and create fact and dimension tables based on the star schema mentioned above. \n",
    "- Run `tl.py`script in the cluster using ssh or copy paste the code in Jupyter Notebook provided by EMR itself.\n",
    "- One can see the output in the output bucket that is created in S3. The data in S3 output folder will be stored as `parquet` format and are partitioned based on the given clauses.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
